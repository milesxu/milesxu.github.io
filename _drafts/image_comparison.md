---
---

# 图片的比较

两个图片的数据相减取绝对值
$$
d_1(I_1,I_2) = \sum_p|I_i^p-I_2^p|
$$
这里 $$ I $$ 代表图片，$$ p $$代表像素。   

## 距离测度：

+ L1（Manhattan）距离： $$ d_1(I_1,I_2) = \sum_p|I_1^p-I_2^p| $$
+ L2（Euclidean）距离： $$ d_2(I_1,I_2) = \sqrt{\sum_p(I_1^p-I_2^p)^2} $$

更广义的Lp标准：
$$
\parallel x\parallel = (|x_1|^p+\cdots+|x_n|^p)^
{\frac{1}{p}}\quad p\geq 1,x\in\mathbb{R}^n
$$

# PCA 算法

+ 从数据中减去均值（将 X 集中）
+ （代表性地）使用各维的方差扩展各维。此法有助于减轻维的量级的影响力（？）
+ 计算协方差矩阵 $$ S $$： $$ S = \frac{1}{N}X^TX $$
+ 计算 $$ S $$的 $$ k $$ 各最大的特征向量
+ 这些特征向量就是 $$ k $$个主成分

# 多维标度测量（Multi-Dimensional Scaling）

目的：寻找一组点的集合，它们之间两两的距离匹配一个给定的距离矩阵   
经典MDS：

+ 给定 $$ n \times n $$ 矩阵，存储的是数据点之间两两的距离。
+ 计算 $$ n \times k $$ 大小的矩阵 $$ X $$ 的坐标距离，可能用到某些线性代数技巧。
+ 对 $$ X $$ 计算PCA

已有应用：地图点距离、图片匹配、Facebook 好友图。

# 正规化

如果要对函数做出一些限制，可以让某些系数为 0，比如限制二项式的最高阶，实际上就是让更高阶的系数为 0。也可以不这么做，还是用较高的阶数，然后对过度拟合做一些软限制，比如设定：
$$
\sum_{i=0}^j a_i^2 < C
$$
可以通过在经验风险函数中添加一个量来达到目的：
$$
\mathcal{R}(h_j) = \sum_{y_i \in \mathcal{D}}(y_i-h_j(x_i))^2
 + \alpha\sum_{i=0}^j a_i^2
$$
新的风险函数比原来的多了一个惩罚项。该项是整数且与系数的平方和成正比。   
此方法就称为正规化或者收缩。使用不为 0 的 $$ alpha $$ 值就是脊回归。目的是限制可选的函数的子集，从而限制了系数过大。

# support vector machine

+ $$ x $$ 是数据点，比如图片，显示猫或狗的图片；
+ $$ y\in\{-1,1\} $$ 是标签，标示是猫还是狗
+ $$ w $$ 是权重向量，定义超平面的
+ $$ w^Tx=0 $$， $$ x $$ 与 $$ w $$ 垂直？
+ $$ w^Tx+b=0 $$， $$ w $$控制超平面朝向， $$ b $$ 控制超平面和原点的距离。这样能够保证超平面在两个类之间能够平移，也能够转动。
+ 有了上述参数的值，剩下的就是按照公式计算，所以不像KNN，不再需要训练数据了。
+ 代价是只能局限于定义的超平面，不像KNN可以进行更多情况的分类。
+ 这是一个神经元的简单数学模型。这些感知机也是深度学习的基础单元。
+ XOR问题：不能被感知机分类？在二维不行。三维即可，加一个维，同时给某个值加一个高度。这样就可用一个平面分类。
+ 和PCA的联系：PCA是要去除噪声的维度。而此时是要加入有用信息的维度，多加维度当然可以，但是没必要。
+ 最大边缘分类 Maximum Margin Classification，要使得将挨得最近的点之间的边缘空隙最大化。这些最近的点称为支撑向量。
+ 有了超平面，有了支撑向量 $$ x^{(i)} $$，从支撑向量向超平面做垂线（投影），焦点即 $$ x_\perp^{(i)} $$。而支撑向量到超平面的法线距离为 $$ \gamma^{(i)} $$。
+ 将 $$ w $$ 规范化，变成长度为1的向量： $$ \frac{w}{\parallel w\parallel} $$。
+ 边缘长度：$$ x_{\perp}^{(i)}=x^{(i)}-\gamma^{(i)}\cdot\frac{w}{\parallel w \parallel} $$
+ 而 $$ x_\perp^{(i)} $$ 满足 $$ w^Tx_\perp^{(i)}+b=0 $$
+ 联立可得 $$ \gamma^{(i)} = \left(\frac{w^Tx^{(i)}+b}{\parallel w \parallel}\right) $$
+ SVM 的目标即发现 $$ w $$ 和 $$ b $$，使得 $$ \gamma^{(i)} $$ 的值最大。
+ 加上 $$ y^{(i)} $$，主要目的是符号和区分：$$ \gamma^{(i)} = y^{(i)}\left(w^Tx+b\right) $$
+ $$ y^{(i)}\left(w^Tx^{(i)}+b\right) \geq \gamma, \quad i=1,\cdots,m $$，有 $$ \parallel w \parallel=1 $$
+ SVM的奇怪之处：为了得到所求，必须要找出两个类中最相近的个体，比如苹果和橘子分类，要找出最像橘子的苹果和最像苹果的橘子。
+ $$ \xi_i $$：松弛变量（slack variables） 是因为少数分类不清的个体，或者是错误标记的个体准备的。
+ $$ \mathrm{min}_{w,b,\xi}\frac{1}{2}\parallel w \parallel^2+\,C\textstyle\sum_{i=1}^n\xi_i $$ 受制于 $$ y^{(i)}\left(w^Tx^{(i)}+b\right) \geq 1-\xi_i, \, (i=1,\cdots,n) $$
+ 如果 $$ C $$ 很大，则允许的松弛变量很小。
+ 加维度的方法，自然是高维的量是从低维的向量计算得到的，比如算平方。没有加数据，但是加了维度。这也使得 SVM 可以处理非线性的决策平面。
+ 二次方核 $$ x=(x_1,x_2) $$ 可做的扩展包括 $$ \Phi(x)=(1,\sqrt2x_1,\sqrt2x_2,x_1^2,x_2^2,   \sqrt2x_1x_2) $$
+ 更高维的： $$ \textstyle\Phi(x)\cdot\Phi(z) = 1+2\sum_{i=1}^dx_iz_i + \sum_{i=1}^dx_i^2z_i^2 + 2\sum_{i=1}^d\sum_{j=i+1}^dx_ix_jz_iz_j = (1+x\cdot z)^2 $$
+ 可以不用计算高维度中的点，来得到包括了高维度的叉乘，避免了大量的计算。核函数 $$ K(x,z)=\Phi(x)\cdot\Phi(z) $$ 即多项式 $$ K(x,z)=(1+x\cdot z)^s $$。 $$ s > 2 $$也成立。
+ 径向基函数（RBF: Radial basis function） $$ K(x,z) = \mathrm{exp}(-\gamma(x-z)^2) $$，最后的指数项也能扩展到无限维。
+ 同样地，也是和 KNN 不同，这里只需要对支撑向量进行计算即可。
+ SVM 不是缩放不变的，所以要检查函数库是不是默认正规化的，要正确地正规化数据，同时以同样方式正规化测试数据。
+ RBF 和是一个很好的默认选择，如果不知道应该怎么选的话。
+ 多个类，可以一对多地分组，即把一组作为一个类，另外所有其他都是一个类，如此循环。
$$
\textstyle\mathrm{max}_\alpha\sum_{i=1}^{m}
\alpha_i-\frac{1}{2}\sum_{i,j=1}^my^{(i)}
y^{(j)}\alpha_i\alpha_jx^{(i)^T}x^{(j)}
$$
最好两项即 $$ K(x^{(i)},x^{(j)}) $$  
$$
\alpha_i \geq 0,\,i=1,\cdots,m
$$
$$
\textstyle\sum_{i=1}^m\alpha_iy^{(i)}=0
$$
$$
\mathrm{arg\,min}_{w,b}\frac{1}{2}
\parallel w \parallel^2
$$
$$
y^{(i)}(w^Tx^{(i)}+b) \geq 1
$$
预测   
$$
w^Tx+b = \sum_{i=1}^m\alpha_iy^{(i)}
\langle x^{(i)},x \rangle +b
$$
F-measure   
$$
F_\beta = \frac{(\beta^2+1)\cdot P\cdot R}
{\beta^2\cdot P+R}
$$

# 决策树和随机森林

+ 如果是两个特性 $$ x_1,x_2 $$，则树的建立过程是交替对两个特性进行比较，从而在空间中划分出不同的区域。高维应该也类似。
+ 看来能够划分出多个区域的，都能够用决策树，然后看如何通其他方法结合。
+ 划分的好坏评价： Node Purity。最好自然是一个叶节点内只有一个分类。如果能够做到，那么同时也是决策树构造的终止信号。
+ 应该怎么解释误分类的算法？ 10 个点， 4 个红、 3 个绿、 3 个蓝，是取出红点的概率，但却误认为是另外两种点的概率的积。
+ Gini Impurity，就是上述所有点的误分类概率的和。一般化：设类别的总数是 $$ C $$，数据点的总数是 $$ N $$，每种数据点的个数是 $$ N_i $$，则 $$ I_G = \textstyle\sum_{i=1}^C\frac{N_i}{N}(1-\frac{N_i}{N}) $$。Gini Impurity 的曲线和熵的曲线很类似。
+ 利用 Gini Impurity 来平均分类的好坏： $$ \bigtriangleup I_G = I_G(A) - \frac{N(B)}{N(A)}I_G(B) - \frac{N(C)}{N(A)}I_G(C) $$。A 是起点，B 和 C 分别是分类后得到的两个节点。
+ 这个方法可以作为构建决策树时分类的指标，选择 Gini 指数最大的分类方式。每次选择当前最大是不是最优的？有没有必要看得更长远？也可以用交叉验证来检验及决定有无必要终止树的构造。其他终止的条件可以是树的高、叶节点数据总数、Gini 指数。
+ 为避免过度拟合，可以先生成树，然后进行一些修剪。那么如果生成的树不是最优的，不管的结构还是分类效果，可以像对待其他树那样进行调整吗？
+ 缺点：对数据的微小改动很敏感、容易过度拟合、只能进行跟轴线对齐的分割，因为每次只取一个特性。
+ 决策树对噪声不敏感，因为每次计算了最优分割。因为是判断，所以可以做的种类比 SVM 要多。
+ 单个决策树的表现并不是很好，但是很快。那么多个呢？但要保证他们之间是不同的。
+ bootstrap 会出现数据的重复、数据集的重叠、缺失，不应该用来做交叉验证。不要简单地用 bootstrap 从同一数据集生成训练和测试数据。
+ bagging: bootstrap aggregating，通过对数据集的元素进行替换获得新的取样，用每个 bootstrap 取样学习一个分类器，然后将结果平均。思想是在得到小的方差的同时，不提高偏差。
+ bagging 减少了过度拟合（方差）、通常使用同类的分类器，决策树非常受欢迎，对线性模型没有帮助，很容易并行化。一般 bagging 是为了减小方差，而线性模型并没有很高的方差。决策树的方差很高。
+ 决策树的多个叶节点可以是同一个分类。
+ 随机森林，建立在 bagging 的思想之上，每个树都建立在 bootstrap 取样之上，节点的分割从随机特性子集计算。
+ 训练多个树，树之间用的训练数据不是相关的，那么就可以达到减小方差的作用。而bagging可以做到让不同组的数据无关。

# ensemble learning and random forests

+ bagging 和 random forest 的差别就是特性的选择。后者除了使用 bootstrap，还会选择随机的特性子集来计算分支。
+ 随机森林在数据中包含很多垃圾的时候表现很差，类似“皇帝的鼻子”故事。
+ 应该根据噪音的多少决定每个树需要多少特性来构建。
+ bootstrap 每次会只用 65% 左右的数据构建单个树，那么剩下的数据就可以对该树进行验证。所有树验证的误差加起来就是 bag 的误差。可用此来观察森林的增长对误差的影响，*但是不能代替交叉验证*。
+ 随机森林的误差依赖于树之间的相关性，越高越差，单个树的强度，越高越好；提高了每个分支的特性数量会增加相关性也会增加单个树的强度。
+ regression 主要是平均，而 classification 则要用到各种形式的 majority vote。
+ regression 的对象多是连续的值，而 classification 的对象必是离散的，给平均值无意义，只有分类，输出只是一个 label。
+ KNN 也可以做 regression，而且 KNN 中也可以加入权重，根据距离。和 regression 联系得比较多的是 prediction。
+ 绝大多数做分类的算法也可以做回归，比如 KNN、 Decision Tree。 SVM 也不是不能做回归，只是比较麻烦。利用 boosting 做的（多个）决策树也可以做回归。
+ 对于 KNN，因为计算量的问题，the curse of dimensionality，应该是需要最小的取样量和特性数量。
+ SVM 可以非常好地应对大量特性的情况，而取样量可以比 KNN 多点，但是可以比特性数量少很多。
+ KNN 在取样量太大时计算量太大，但是 random forest 则可以应对取样量很大的情况，因为每个树的计算是独立的，且可以并行完成。但是 random forest 应对特性数量上没有 SVM 优秀。
+ 而在取样量和特性数量都非常大的时候，就应该用深度学习 deep learning。
+ 正规化，可以是先分组：train、 validation、 test 各组自己进行求均值然后正规化，只适用于取样量很大，各组特别是测试组的样本数量很多的情况；也可以先只对训练数据正规化，这是所有数据中数量最大的，然后利用训练数据的均值来正规化所有三个组，包括训练数据本身、验证数据、测试数据。
+ 交叉验证的前提是数据都是独立的，不相关的。如果是相关的数据，则要进行预处理，保证不同的数据之间是不相关的。简单地 shuffle 数据并不能有效。可以把数据不同部分混合起来。
+ 对不平衡的类进行交叉验证：使用分层取样的方式来生成各组数据，目的就是让 train、 validation、 test 数据集中的类的比例相同。可以对占比较多的数据进行二次取样，降低其数量；可以对占比很少的数据进行过采样，利用 bootstrap 等方法。
+ 缺失数据：可以删除数据点；可以使用特性的均值，也就是不改变取样均值，但是却和其他特性独立；可以使用回归来估计值。
+ collaborative filtering 有很多角度，关联的角度是喜欢的东西类似的用户，可以通过此推断某些缺失的东西的喜欢程度；仅仅只从用户的 ranking 出发，但是可以是许多件不同领域不同物品的 ranking。
+ content based filtering 每个物品用一系列的特性描述，衡量不同物品间的相似度，推荐和用户喜欢的物品相似的东西；后者在用户比物品多的时候比较好，而且物品最好是相对稳定的，而用户可以变化。预测的东西是在用户的 comfort zone 之内的。
+ 用回归做 CF：选择最喜欢的回归算法，为每个物品训练一个预测器，每个用户评价一个物品，就提供了一个取样，对一个物品 A 预测一个评价，使用 A 的预测器，参数则是用户的不完整的评价向量。优点：将推荐降低到一个被深入研究的问题，有许多非常好的预测算法可用；缺点：必须处理非常非常大量的缺失数据，训练 M 个预测器开销很大。
+ 用 KNN 做 CF：使用非常广泛，基于物品或者用户都行，将每个用户表示为不完整的向量，内容是对物品的评价，计算被查询用户和所有其他用户的相似度，在已对被查询的物品提供了评价的用户中，查找 K 个最相似的用户，预测评价的权重平均。
+ 相似度的衡量：皮尔森相关系数、余弦相似度。
+ MapReduce 每次做完 Map 之后，会将结果写入硬盘，所以 I/O 并不是很效率。
+ SVD：$$ A_{[m\,x\,n]} = U_{[m\,x\,r]}\Sigma_{[r\,x\,r]}\left(V_{[n\,x\,r]}\right)^T $$
+ A: Input data matrix, m x n matrix (e.g., m documents, n terms)
+ U: Left singular vectors, m x r matrix (m documents, r concepts)
+ $$ \Sigma $$: Sigular values, r x r diagonal matrix (strength of each 'concept'), r: rank of the matrix A.
+ V: Right sigular vectors, n x r matrix (n terms, r concepts)
+ $$ A \approx U\Sigma V^T = \Sigma_i\sigma_iu_i \circ v_i^T $$
+ 对于任何实数矩阵，都有且唯一的 SVD 分解
+ $$ U^T\,U = I, \, V^T V = I $$，U 和 V 都是列正交矩阵。
+ $$ \Sigma $$ 是对角矩阵，对角元素都是正的，而且降序排列。

# MapReduce 和 Sparks

+ mapper, combiner, reducer
+ combiner 并不会保证被执行！所以 combiner 的返回值应该和 mapper 有相同的形式，这样才能与没有做过 combiner 而直接输出的数据一起进入 reducer。而因为和 reducer 一样都处理来自 mapper 的数据，所以函数签名又和 reducer 一样。
+ 函数式编程对并行计算有好处的原因是，线程间有共享的状态是非常麻烦和危险的事情。同时如果有某台机器宕机，其生成的数据可以很方便地在其他机器上生成，只要他是数据流图中的一个节点。

# Bayesian

有关条件概率的定义：
$$
p(h|D) = \frac{p(D|h)p(h)}{p(D)}
$$
读作：
$$
Posterior=\frac{Likelihood \times Prior}{Evidence}
$$
定义
$$
E_{XY}[f(X,Y)] = \sum_x\sum_y f(x,y)p(x,y)dxdy
$$
这里 $$ p $$ 是特性 $$ x $$ 和响应 $$ y $$ 的联合概率密度。
$$
R_g(x) = \sum_yl(y,g(x))p(y|x)
$$
这里我们计算对于 $$ y $$ 的所有选择的平均风险，$$ g $$ 作为给定的取样。   
如果我们要计算总体风险，基于我们已有的所有取样：
$$
\begin{align}
R(g) &= \sum_x p(x)R_g(x) = \sum_x p(x)\sum_y l(y,g(x))p(y|x) \\
&= \sum_x\sum_yl(y,g(x))p(x)p(y|x) = \sum_x\sum_yl(y,g(x))p(x,y)
\end{align}
$$
和 $$ g $$ 相关的风险：
$$
R(g) = E_{XY}[l(g(X), Y)]
$$
中括号中的项即 loss，当特性 $$ x $$ 时 $$ Y = y $$ 我们选择 $$ g $$。可以是平方差，对于回归，也可以是错误分类次数，对于分类。
我们希望得到优化的决策函数来最小化风险（或最大化利用率）。但如果我们不知道 $$ p $$，且对于不同的机器学习形式有不同的方式来估计 $$ p $$。仅在我们估计的分布跟真实的 $$ p $$ 时我们期望我们的决策能让我们将基于训练的成果很好地向外推广。
需要做的：

+ 估计密度
+ 最小化风险

我们选择某个参数化模型 $$ g(x;\theta) $$，然后最小化风险得到 $$ \theta_{opt} $$，但是在应用到训练集之外时，很容易过渡拟合，所以经常添加一个规则化项，其系数为 $$ \lambda $$。   
缺点：当数据集很小时很难得到比较好的分布，而且如果数据变化了导致 loss 变化了，需要重新估计 $$ g $$。   
贝叶斯的方法是首先进行密度估计，估计联合概率密度 $$ p(x,y) $$ 或者 $$ p(x,c) $$。discrminative 方法：
$$
p(x,c)=p(c|x)p(x)
$$
generative 方法：
$$
p(x,c)=p(x|c)p(c)
$$
generative 分类器：   
以性别数据为例，我们完全有理由假定男性和女性的身高/体重数据各自符合某个正态分布。那么就可以把男性和女性的测试数据分开，然后分别拟合出参数。   
协方差矩阵：两个特性各自的方差，因为方差就是点的离散程度，所以协方差矩阵可以作为划范围的半径，或者椭圆的长短轴。而均值就是其中心。   
这样就有了男性的贝叶斯模型，也有女性的。然后将新的值代入这两个模型中，看其更可能是哪一类。   
从另一个角度思考这个问题，即我们 *生成（generate）* 或模拟了一个新的男性或女性取样。   
这样做的三个特点：对不同类进行单独建模；公式中涉及到了 $$ P(y) $$，取样为某一类的先验概率；我们需要提供更多数据。  
这样的好处是可以很好地处理分类不平衡的数据集。此时有可能对于 discriminative classifier 很难去发现精确的边界。   
用此法绘制的概率等高线图和前面的一致，事实上，LDA 和 Logistics 回归是对应的。而且 Poisson Likelihood、Naive Bayes 都有 Logistics 回归的 discriminative 对应。因此 Logistics 回归是相对健壮的模型，对很多建模假设不敏感。这是使用 discriminative 模型的一大原因。   
而如果 $$ p(x|y) $$ 的确是正态的，如上的例子，那么 LDA 是渐进式的高效。对于有大量训练数据的情形，可以证明没有模型可以比 LDA 对概率进行更好的估计。特别地，可以显示 LDA 超过了 Logistics 回归，哪怕是对于规模较小的训练集。   
generative 分类器的另一个好处，可以将各特性基于类的条件概率同先验概率（作为权重）相乘然后求和，用边缘化概率密度公式估计输入密度。   

## Naive Bayes

Naive Bayes 是一个简单的 generative 模型，使用了疯狂的独立条件假设。其从类的先验开始，且认为：
$$
p(x_1,x_2|c) = p(x_1|c)p(x_2|c)
$$
$$ x_1 $$ 和 $$ x_2 $$ 在 $$ C $$ 上独立。该独立条件允许我们“组合模型”。   

+ Naive Bayes 对于诸如 cancer 取样的多种检查是很好的假设。组织活检和血液检查是独立的。   
+ 对于文档，可以想象每次针对一个单词，但是针对作者们而言是很差的 generative 模型。   
+ 对于文字分类， naive bayes 表现比决策树归纳要好。SVM 在这方面又比 naive bayes 好。
+ 作为一个经验法则，对任何问题首先尝试 KNN 和 NB。
+ 缺点是如果出现 0 个就不会估计了。
+ 用 log 来计算，否则会有向下溢出。不要概率相乘，要做 log 值的和。

## generative 和 discriminative 的比较

+ 如果只需要做分类决策，而不是更多复杂决策，discriminative 就够了，可以节省计算资源。不对称的数据和生成人造数据是 generative 的长处。
+ LDA 和 NB 在某些情况下拟合非常简单。Logistics 回归需要经由梯度下降的凸优化（convex optimization via Gradient descent）。
+ 我们能向 generative 分类器中加入新类而不对之前的类进行再训练，所以可能更适合在线用户选择问题，用户的信息会更改。
+ generative 分类器能更容易地处理缺失数据。
+ generative 分类器在处理未打标签的训练数据更好（semi-supervized learning）。
+ 使用 discriminative 分类器需要的数据预处理更简单。
+ 通常 discriminative 分类器有更好的校准概率。

+ 频率论者的方法，是使用某些函数对数据 $$ D $$ 的一个参数进行估计，他们认为该参数的真实值是固定的，然而数据是随机的。即对于种群而言有个真实参数 $$ \theta^{\star} $$。通过多次重复使用我们可以得到 $$ \theta $$ 的取样分布，用来估算参数估计的误差。进行参数估计的基本方法是最大似然估计（MLE）。因为最大似然估计希望将训练集中的每个点都计算在内，所以经常过度拟合。
+ Bayesian 的方法是，将 $$ \theta $$ 看作是一个随机变量，而固定数据集。所以我们不再把数据集看作是种群的取样，而是我们所知的世界。然后我们将 $$ \theta $$ 和一个先验分布 $$ p(\theta) $$ 联系起来。先验分布表示了我们对还未观察的数据的参数值的信念。
+ Bayesian 的层状模型，就是先所有数据进行一个平均，获得某个先验均值。所有区域使用相同的分布模型，但是模型的参数各不相同。但是，所有这些参数都从一个模型中得出，一般是正态分布。
+ 为对一个测试情况进行预测，我们平均所有可能的参数预测分布值，以它们各自的后验概率作为权重。
+ 绘制期望值和观测值的对应曲线，则轨迹越接近 45 度过原点的直线，效果越好。
+ clustering 是一种非监视的学习方式，其目的是发现隐藏在数据中的结构。
